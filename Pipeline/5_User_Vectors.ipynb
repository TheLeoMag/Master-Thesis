{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a17819b-b5d3-4474-83a0-86117e9ed358",
   "metadata": {},
   "source": [
    "# V User Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a912f3-4465-4c93-bfed-d39446b0d4e8",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Loading the Data and Necessary Libraries](#loading-dependencies)\n",
    "2. [Proximity Prestige & Degree Centrality ](#prestige)\n",
    "3. [Comment Quality](#quality)\n",
    "4. [Save Results](#save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125933d7-f08c-49da-845e-fe9745a61863",
   "metadata": {},
   "source": [
    "## Loading Data and Libraries \n",
    "<a class=\"anchor\" id=\"loading-dependencies\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b110491-f3c8-407e-bb30-f0879c1e592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "df_c             = pd.read_parquet('Comments.parquet')\n",
    "Explicit_links   = pd.read_parquet('explicit_links.parquet')          \n",
    "vector_values_df = pd.read_parquet('Opinion_rank_scores.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf3636-c7c7-4a59-91fe-f89097a0a1f6",
   "metadata": {},
   "source": [
    "## Proximity Prestige & Degree Centrality \n",
    "<a class=\"anchor\" id=\"prestige\"></a>\n",
    "\n",
    "The graph user network is constructed for each article. While the graph is in memory,  \n",
    "the Proximity Prestige and Degree Centrality calculations are performedfor each user in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a5fa00-67cb-4dae-9fe2-d5fdaf145e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 16787/16787 [2:20:02<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "-Iterates over each article \n",
    "-Builds a Graph User Network of each article\n",
    "-Computes Degree Centrality & Proximity Presige for each User\n",
    "-Maps DC & PP back to the users in vector_values_df\n",
    "'''\n",
    "\n",
    "def calculate_proximity_prestige(G):\n",
    "    pp_score = {}\n",
    "    N = G.number_of_nodes()\n",
    "\n",
    "    for i in G.nodes():\n",
    "        reachable_nodes = nx.single_source_shortest_path_length(G, i)\n",
    "        reachable_count = len(reachable_nodes) - 1  # excluding the node itself\n",
    "        \n",
    "        if reachable_count > 0:\n",
    "            total_distance = sum(reachable_nodes.values())  # total distance to the node i from all reachable nodes\n",
    "            average_distance = total_distance / reachable_count\n",
    "            pp_score[i] = (reachable_count / (N - 1)) / average_distance\n",
    "        else:\n",
    "            pp_score[i] = 0\n",
    "\n",
    "    return pp_score\n",
    "    \n",
    "vector_values_df['degree_centrality'] = pd.Series(dtype=float)\n",
    "vector_values_df['proximity_prestige'] = pd.Series(dtype=float)\n",
    "\n",
    "for article in  tqdm(vector_values_df.articleID.unique()):\n",
    "\n",
    "    df = Explicit_links[Explicit_links.articleID == article]\n",
    "    df = df.dropna()\n",
    "\n",
    "    G=nx.from_pandas_edgelist(df, 'user_ID_a','user_ID_b' ,create_using=nx.DiGraph())\n",
    "\n",
    "    degree_centrality = nx.out_degree_centrality(G)\n",
    "    proximity_prestige = calculate_proximity_prestige(G)\n",
    "    \n",
    "    vector_values_df.loc[vector_values_df.articleID == article, \"degree_centrality\"] = vector_values_df.loc[vector_values_df.articleID == article, \"userID\"].map(degree_centrality)\n",
    "    vector_values_df.loc[vector_values_df.articleID == article, \"proximity_prestige\"] = vector_values_df.loc[vector_values_df.articleID == article, \"userID\"].map(proximity_prestige)\n",
    "    #print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ea0ce-6806-40cf-b627-f0835ba3e674",
   "metadata": {},
   "source": [
    "## Comment Quality\n",
    "<a class=\"anchor\" id=\"quality\"></a>\n",
    "\n",
    "The comment_quality is calculated from the previously computed Opinion_rank_score_sum and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077c7bd4-d7b4-45c8-95ae-e4ee1b115ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reads in df_c and groups by article and user.\n",
    "- Counts each user's comment count per article.\n",
    "- Calculates the average length of each user's comments per article.\n",
    "- Identifies the maximum average comment length across all users per article.\n",
    "- Merges these metrics back into the vector_values_df for further analysis.\n",
    "\"\"\"\n",
    "\n",
    "number_of_comments_per_user = df_c.groupby(['articleID', 'userID']).size().reset_index(name='comment_count')\n",
    "\n",
    "grouped_df = df_c.groupby(['articleID', 'userID'])\n",
    "df_c['CommentLength'] = df_c['commentBody'].apply(len)\n",
    "\n",
    "average_length_per_user = grouped_df['CommentLength'].mean()\n",
    "average_lengt_user_comment = average_length_per_user.reset_index(name='average_comment_lenght')\n",
    "\n",
    "max_length_per_article = average_lengt_user_comment.groupby('articleID')['average_comment_lenght'].max()\n",
    "max_length_per_article = max_length_per_article.reset_index(name='max_comment_length')\n",
    "\n",
    "vector_values_df = pd.merge(vector_values_df, number_of_comments_per_user, on=[\"articleID\", \"userID\"], how=\"left\")\n",
    "vector_values_df = pd.merge(vector_values_df, average_lengt_user_comment, on=[\"articleID\", \"userID\"], how=\"left\")\n",
    "vector_values_df = pd.merge(vector_values_df, max_length_per_article, on=[\"articleID\"], how=\"left\")\n",
    "\n",
    "vector_values_df['comment_quality'] = ((vector_values_df['Opinion_rank_score_sum']/vector_values_df['comment_count'])*\n",
    "                                       (vector_values_df['average_comment_lenght']/vector_values_df['max_comment_length']))\n",
    "vector_values_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b450ad-f38b-47e0-9f5d-0ea95d479a5a",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "<a class=\"anchor\" id=\"save\"></a>\n",
    "\n",
    "The filtered dataframe will be exported as a Parquet file, with NaN values set to zero to retain isolated comments and preserve their Opinion_rank_score_sum for indirect influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0afc3d-6e23-4630-99b6-a385d003c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We set nan values to zer0, this means that we do not drop any comments that are isolated in the GUN.\n",
    "Tropping them would mean thath we loose the Opinion_rank_score_sum which would still contain the info in\n",
    "case the comennt had influenced other indirectly \n",
    "\n",
    "- Exports the filtered dataframe to a Parquet.\n",
    "'''\n",
    "df = vector_values_df.fillna(0)\n",
    "df.to_parquet('user_Vectors.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
